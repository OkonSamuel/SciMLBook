<h1 class=title >Global Sensitivity Analysis</h1> <h5>Chris Rackauckas</h5> <h5>December 12st, 2020</h5> <h2><a href="https://youtu.be/wzTpoINJyBQ">Youtube Video</a></h2> <p>Sensitivity analysis is the measure of how sensitive a model is to changes in parameters, i.e. how much the output changes given a change in the input. Clearly, derivatives are a measure of sensitivity, but derivative are <em>local sensitivity</em> measures because they are only the derivative at a single point. However, the idea of probabilistic programming starts to bring up an alternative question: how does the output of a model generally change with a change in the input? This kind of question requires an understanding of <em>global sensitivity</em> of a model. While there isn&#39;t a single definition of the concept, there are a few methods that individuals have employed to estimate the global sensitivity.</p> <p>Reference implementations of these methods can be found in <a href="https://github.com/SciML/GlobalSensitivity.jl">GlobalSensitivity.jl</a></p> <h2>Setup for Global Sensitivity</h2> <p>In our global sensitivity analysis, we have a model <span class=math >$f$</span> and want to understand the relationship</p> <p class=math >\[ y = f(x_i) \]</p> <p>Recall <span class=math >$f$</span> can be a neural network, an ODE solve, etc. where the <span class=math >$X_i$</span> are items like initial conditions and parameters. What we want to do is understand how much the total changes in <span class=math >$y$</span> can be attributed to changes in specific <span class=math >$x_i$</span>.</p> <p>However, this is not an actionable form since we don&#39;t know what valid inputs into <span class=math >$f$</span> look like. Thus any global sensitivity study at least needs a domain for the <span class=math >$x_i$</span>, at least in terms of bounds. This is still underdefined because what makes one thing that it&#39;s not more likely for <span class=math >$x_i$</span> to be near the lower part of the bound instead of the upper part? Thus, for global sensitivity analysis to be well-defined, <span class=math >$x_i$</span> must take a distributional form, i.e. be random variables. Thus <span class=math >$f$</span> is a deterministic program with probabilistic inputs, and we want to determine the effects of the distributional inputs on the distribution of the output.</p> <h3>Reasons for Global Sensitivity Analysis</h3> <p>What are the things we can learn from doing such a global sensitivity analysis?</p> <ol> <li><p>You can learn what variables would need to be changed to drive the solution in a given direction or control the system. If your model is exact and the parameters are known, the &quot;standard&quot; methods apply, but if your model is only approximate, a global sensitivity metric may be a better prediction as to how variables cause changes.</p> <li><p>You can learn if there are any variables which do not have a true effect on the output. These variables would be practically unidentifiable from data and models can be reduced by removing the terms. It also is predictive as to robustness properties.</p> <li><p>You can find ways to automatically sparsify a model by dropping off the components which contribute the least. This matters in automatically generated or automatically detected models, where many pieces may be spurious and global sensitivities would be a method to detect that in a manner that is not sensitive to the chosen parameters.</p> </ol> <h2>Global Sensitivity Analysis Measures</h2> <h3>Linear Global Sensitivity Metrics: Correlations and Regressions</h3> <p>The first thing that you can do is approximate the full model with a linear surrogate, i.e.</p> <p class=math >\[ y = AX \]</p> <p>for some linear model. A regression can be done on the outputs of the model in order to find the linear approximation. The best fitting global linear model then gives coefficients for the global sensitivities via the individual effects, i.e. for</p> <p class=math >\[ y = \sum_i \beta_i x_i \]</p> <p>,</p> <p>the <span class=math >$\beta_i$</span> are the global effect. Just as with any use of a linear model, the same ideas apply. The coefficient of determination &#40;<span class=math >$R^2$</span>&#41; is a measure of how well the model fits. However, one major change needs to be done in order to ensure that the solutions are comparable between different models. The dependence of the solution on the units can cause the coefficients to be large/small. Thus we need to normalize the data, i.e. use the transformation</p> <p class=math >\[ \tilde{x_i} = \frac{x_i-E[x_i]}{V[x_i]} \]</p> <p class=math >\[ \tilde{y_i} = \frac{y_i-E[y_i]}{V[y_i]} \]</p> <p>The normalized coefficients are known as the <em>Standardized Regression Coefficients</em> &#40;SRC&#41; and are a measure of the global effects.</p> <p>Notice that while the <span class=math >$\beta_i$</span> capture the mean effects, it holds that</p> <p class=math >\[ V(y) = \sum_i \beta^2_i x_i \]</p> <p>and thus the variance due to <span class=math >$x_i$</span> can be measured as:</p> <p class=math >\[ SRC_i = \beta_i \sqrt{\frac{V[x_i]}{V[y]}} \]</p> <p>This interpretation is the same as the solution from the normalized variables.</p> <p>From the same linear model, two other global sensitivity metrics are defined. The <em>Correlation Coefficients</em> &#40;CC&#41; are simply the correlations:</p> <p class=math >\[ CC_i = \frac{\text{cov}(x_i,y)}{\sqrt{V[x_i]V[y]}} \]</p> <p>Similarly, the <em>Partial Correlation Coefficient</em> is the correlation coefficient where the linear effect of the other terms are removed, i.e. for <span class=math >$S_i = {x_1,x_2,\ldots,x_{j-1},x_{j+1},\ldots,x_n}$</span> we have</p> <p class=math >\[ PCC_{i|S_i} = \frac{\text{cov}(x_i,y|S)j)}{\sqrt{V[x_i|S_i]V[y|S_i]}} \]</p> <h3>Derivative-based Global Sensitivity Measures &#40;DGSM&#41;</h3> <p>To go beyond just a linear model, one might want to do successive linearization. Since derivatives are a form of linearization, then one may thing to average derivatives. This averaging of derivatives is the DGSM method. If the <span class=math >$x_i$</span> are random variables with joint CDF <span class=math >$F(x)$</span>, then it holds that:</p> <p class=math >\[ v_i = \int_{R^d} \left(\frac{\partial f(x)}{\partial x_i}\right)^2 dF(x) = \mathbb{E}\left[\left(\frac{\partial f(x)}{\partial x_i}\right)^2\right], \]</p> <p>We can also define the mean measure, which is simply:</p> <p class=math >\[ w_i = \int_{R^d} \frac{\partial f(x)}{\partial x_i} dF(x) = \mathbb{E}\left[\frac{\partial f(x)}{\partial x_i}\right]. \]</p> <p>Thus a global variance estimate would be <span class=math >$v_i - w_i^2$</span>.</p> <h3>ADVI for Global Sensitivity</h3> <p>Note that the previously discussed method for probabilistic programming, ADVI, is a method for producing a Gaussian approximation for a probabilistic program. The resulting mean-field or full Gaussian approximations are variance index calculations&#33;</p> <h3>The Morris One-At-A-Time &#40;OAT&#41; Method</h3> <p>Instead of using derivatives, one can use finite difference approximations. Normally you want to use small <span class=math >$\Delta x$</span>, but if we are averaging derivatives over a large area, then in reality we don&#39;t really need a small <span class=math >$\Delta x$</span>&#33;</p> <p>This is where the Morris method comes in. The basic idea is that moving in one direction at a time is a derivative estimate, and if we step large enough then the next derivative estimate may be sufficiently different enough to contribute well to the total approximation. Thus we do the following:</p> <ol> <li><p>Take a random starting point</p> <li><p>Randomly choose a direction <span class=math >$i$</span> and make a change <span class=math >$\Delta x_i$</span> only in that direction.</p> <li><p>Calculate the derivative approximation from that change. Repeat 2 and 3.</p> </ol> <p>Keep doing this for enough steps, and the average of your derivative approximations becomes a global index. Notice that this reuses every simulation as part of two separate estimates, making it much more computationally efficient than the other methods. However, it accounts for average changes and not necessarily measurements gives a value that&#39;s a decomposition of a total variance. But its computational cost makes it attractive for making quick estimates of the global sensitivities.</p> <p>For practical usage, a few changes have to be done. First of all, notice that positive and negative change can cancel out. Thus if one want to measure of associated variance, one should use absolute values or squared differences. Also, one needs to make sure that these trajectories get good coverage of the input space. Define the distance between two trajectories as the sum of the geometric distances between all pairs of points. Generate many more trajectories than necessary and choose the <span class=math >$r$</span> trajectories with the largest distance. If the model evaluations are expensive, this is significantly cheap enough in comparison that it&#39;s okay to do.</p> <h3>Sobol&#39;s Method &#40;ANOVA&#41;</h3> <p>Sobol&#39;s method is a true nonlinear decomposition of variance and it is thus considered one of the gold standards. For Sobol&#39;s method, we define the decomposition</p> <p class=math >\[ f(x) = f_0 + \sum_i f_i(x_i) + \sum_{i,j} f_{ij}(x_i,x_j) + \ldots \]</p> <p>where</p> <p class=math >\[ f_0 = \int_\Omega f(x) dx \]</p> <p>and orthogonality holds:</p> <p class=math >\[ f_{i,j,\ldots}(x_i,x_j,\ldots)dx = 0 \]</p> <p>by the definitions:</p> <p class=math >\[ f_i(x_i) = E(y|x_i) - f_0 \]</p> <p class=math >\[ f_{ij}(x_i,y_j) = E(y|x_i,x_j) - f_0 - f_i - f_j \]</p> <p>Assuming that <span class=math >$f(x)$</span> is L2, it holds that</p> <p class=math >\[ \int_\Omega f^2(x)dx - f_0^2 = \sum_s \sum_i \int f^2_{i_1,i_2,\ldots,i_s} dx \]</p> <p>and thus</p> <p class=math >\[ V[y] = \sum V_i + \sum V_{ij} + \ldots \]</p> <p>where</p> <p class=math >\[ V_i = V[E_{x_{\sim i}}[y|x_i]] \]</p> <p class=math >\[ V_{ij} = V[E_{x_{\sim ij}}[y|x_i,x_j]]-V_i - V_j \]</p> <p>where <span class=math >$X_{\sim i}$</span> means all of the variables except <span class=math >$X_i$</span>. This means that the total variance can be decomposed into each of these variances.</p> <p>From there, the fractional contribution to the total variance is thus the index:</p> <p class=math >\[ S_i = \frac{V_i}{Var[y]} \]</p> <p>and similarly for the second, third, etc. indices.</p> <p>Additionally, if there are too many variables, one can compute the contribution of <span class=math >$x_i$</span> including all of its interactions as:</p> <p class=math >\[ S_{T_i} = \frac{E_{X_{\sim i}}[Var[y|X_{\sim i}]]}{Var[y]} = 1 - \frac{Var_{X_{\sim i}}[E_{X_i}[y|x_{\sim i}]]}{Var[y]} \]</p> <h4>Computational Tractability and Quasi-Monte Carlo</h4> <p>Notice that every single expectation has an integral in it, so the variance is defined as integrals of integrals, making this a very challenging calculation. Thus instead of directly calculating the integrals, in many cases Monte Carlo estimators are used. Instead of a pure Monte Carlo method, one generally uses a low-discrepancy sequence &#40;a form of quasi-Monte Carlo&#41; to effectively sample the search space.</p> <p>The following generates for example a <em>Sobol sequence</em>:</p> <pre class='hljl'>
<span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>Sobol</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>Plots</span><span class='hljl-t'>
</span><span class='hljl-n'>s</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>SobolSeq</span><span class='hljl-p'>(</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>p</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>hcat</span><span class='hljl-p'>([</span><span class='hljl-nf'>next!</span><span class='hljl-p'>(</span><span class='hljl-n'>s</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>1024</span><span class='hljl-p'>]</span><span class='hljl-oB'>...</span><span class='hljl-p'>)</span><span class='hljl-oB'>&#39;</span><span class='hljl-t'>
</span><span class='hljl-nf'>scatter</span><span class='hljl-p'>(</span><span class='hljl-n'>p</span><span class='hljl-p'>[</span><span class='hljl-oB'>:</span><span class='hljl-p'>,</span><span class='hljl-ni'>1</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>p</span><span class='hljl-p'>[</span><span class='hljl-oB'>:</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>])</span>
</pre> <pre class=julia-error >
ERROR: Failed to precompile Plots &#91;91a5bcdd-55d7-5caf-9e0b-520d859cae80&#93; to &quot;/home/runner/.julia/compiled/v1.10/Plots/jl_cf05VD&quot;.
</pre> <p>Another common quasi-Monte Carlo sequence is the <em>Latin Hypercube</em>, which is a generalization of the Latin Square where in every row, column, etc. only one point is given, allowing a linear spread over a high dimensional space.</p> <pre class='hljl'>
<span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>LatinHypercubeSampling</span><span class='hljl-t'>
</span><span class='hljl-n'>p</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>LHCoptim</span><span class='hljl-p'>(</span><span class='hljl-ni'>120</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-ni'>1000</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>scatter</span><span class='hljl-p'>(</span><span class='hljl-n'>p</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>][</span><span class='hljl-oB'>:</span><span class='hljl-p'>,</span><span class='hljl-ni'>1</span><span class='hljl-p'>],</span><span class='hljl-n'>p</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>][</span><span class='hljl-oB'>:</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>])</span>
</pre> <pre class=julia-error >
ERROR: Failed to precompile LatinHypercubeSampling &#91;a5e1c1ea-c99a-51d3-a14d-a9a37257b02d&#93; to &quot;/home/runner/.julia/compiled/v1.10/LatinHypercubeSampling/jl_l9WwPc&quot;.
</pre> <p>For a reference library with many different quasi-Monte Carlo samplers, check out <a href="https://github.com/SciML/QuasiMonteCarlo.jl">QuasiMonteCarlo.jl</a>.</p> <h2>Fourier Amplitude Sensitivity Sampling &#40;FAST&#41; and eFAST</h2> <p>The FAST method is a change to the Sobol method to allow for faster convergence. First transform the variables <span class=math >$x_i$</span> onto the space <span class=math >$[0,1]$</span>. Then, instead of the linear decomposition, one decomposes into a Fourier basis:</p> <p class=math >\[ f(x_i,x_2,\ldots,x_n) = \sum_{m_1 = -\infty}^{\infty} \ldots \sum_{m_n = -\infty}^{\infty} C_{m_1m_2\ldots m_n}\exp\left(2\pi i (m_1 x_1 + \ldots + m_n x_n)\right) \]</p> <p>where</p> <p class=math >\[ C_{m_1m_2\ldots m_n} = \int_0^1 \ldots \int_0^1 f(x_i,x_2,\ldots,x_n) \exp\left(-2\pi i (m_1 x_1 + \ldots + m_n x_n)\right) \]</p> <p>The ANOVA like decomposition is thus</p> <p class=math >\[ f_0 = C_{0\ldots 0} \]</p> <p class=math >\[ f_j = \sum_{m_j \neq 0} C_{0\ldots 0 m_j 0 \ldots 0} \exp (2\pi i m_j x_j) \]</p> <p class=math >\[ f_{jk} = \sum_{m_j \neq 0} \sum_{m_k \neq 0} C_{0\ldots 0 m_j 0 \ldots m_k 0 \ldots 0} \exp \left(2\pi i (m_j x_j + m_k x_k)\right) \]</p> <p>The first order conditional variance is thus:</p> <p class=math >\[ V_j = \int_0^1 f_j^2 (x_j) dx_j = \sum_{m_j \neq 0} |C_{0\ldots 0 m_j 0 \ldots 0}|^2 \]</p> <p>or</p> <p class=math >\[ V_j = 2\sum_{m_j = 1}^\infty \left(A_{m_j}^2 + B_{m_j}^2 \right) \]</p> <p>where <span class=math >$C_{0\ldots 0 m_j 0 \ldots 0} = A_{m_j} + i B_{m_j}$</span>. By Fourier series we know this to be:</p> <p class=math >\[ A_{m_j} = \int_0^1 \ldots \int_0^1 f(x)\cos(2\pi m_j x_j)dx \]</p> <p class=math >\[ B_{m_j} = \int_0^1 \ldots \int_0^1 f(x)\sin(2\pi m_j x_j)dx \]</p> <h4>Implementation via the Ergodic Theorem</h4> <p>Define</p> <p class=math >\[ X_j(s) = \frac{1}{2\pi} (\omega_j s \mod 2\pi) \]</p> <p>By the ergodic theorem, if <span class=math >$\omega_j$</span> are irrational numbers, then the dynamical system will never repeat values and thus it will create a solution that is dense in the plane &#40;Let&#39;s prove a bit later&#41;. As an animation:</p> <p><img src="https://upload.wikimedia.org/wikipedia/commons/6/64/Search_curve_1.gif" alt="" /></p> <p>&#40;here, <span class=math >$\omega_1 = \pi$</span> and <span class=math >$\omega_2 = 7$</span>&#41;</p> <p>This means that:</p> <p class=math >\[ A_{m_j} = \lim_{T\rightarrow \infty} \frac{1}{2T} \int_{-T}^T f(x)\cos(m_j \omega_j s)ds \]</p> <p class=math >\[ B_{m_j} = \lim_{T\rightarrow \infty} \frac{1}{2T} \int_{-T}^T f(x)\sin(m_j \omega_j s)ds \]</p> <p>i.e. the multidimensional integral can be approximated by the integral over a single line.</p> <p>One can satisfy this approximately to get a simpler form for the integral. Using <span class=math >$\omega_i$</span> as integers, the integral is periodic and so only integrating over <span class=math >$2\pi$</span> is required. This would mean that:</p> <p class=math >\[ A_{m_j} \approx \frac{1}{2\pi} \int_{-\pi}^\pi f(x)\cos(m_j \omega_j s)ds \]</p> <p class=math >\[ B_{m_j} \approx \frac{1}{2\pi} \int_{-\pi}^\pi f(x)\sin(m_j \omega_j s)ds \]</p> <p>It&#39;s only approximate since the sequence cannot be dense. For example, with <span class=math >$\omega_1 = 11$</span> and <span class=math >$\omega_2 = 7$</span>:</p> <p><img src="https://upload.wikimedia.org/wikipedia/commons/2/29/Search_curve_3.gif" alt="" /></p> <p>A higher period thus gives a better fill of the space and thus a better approximation, but may require a more points. However, this transformation makes the true integrals simple one dimensional quadratures which can be efficiently computed.</p> <p>To get the total index from this method, one can calculate the total contribution of the complementary set, i.e. <span class=math >$V_{c_i} = \sum_{j \neq i} V_j$</span> and then</p> <p class=math >\[ S_{T_i} = 1 - S_{c_i} \]</p> <p>Note that this then is a fast measure for the total contribution of variable <span class=math >$i$</span>, including all higher-order nonlinear interactions, all from one-dimensional integrals&#33; &#40;This extension is called extended FAST or eFAST&#41;</p> <h4>Proof of the Ergodic Theorem</h4> <p>Look at the map <span class=math >$x_{n+1} = x_n + \alpha (\text{mod} 1)$</span>, where <span class=math >$\alpha$</span> is irrational. This is the irrational rotation map that corresponds to our problem. We wish to prove that in any interval <span class=math >$I$</span>, there is a point of our orbit in this interval.</p> <p>First let&#39;s prove a useful result: our points get arbitrarily close. Assume that for some finite <span class=math >$\epsilon$</span> that no two points are <span class=math >$\epsilon$</span> apart. This means that we at most have spacings of <span class=math >$\epsilon$</span> between the points, and thus we have at most <span class=math >$\frac{2\pi}{\epsilon}$</span> points &#40;rounded up&#41;. This means our orbit is periodic. This means that there is a <span class=math >$p$</span> such that</p> <p class=math >\[ x_{n+p} = x_n \]</p> <p>which means that <span class=math >$p \alpha = 1$</span> or <span class=math >$p = \frac{1}{\alpha}$</span> which is a contradiction since <span class=math >$\alpha$</span> is irrational.</p> <p>Thus for every <span class=math >$\epsilon$</span> there are two points which are <span class=math >$\epsilon$</span> apart. Now take any arbitrary <span class=math >$I$</span>. Let <span class=math >$\epsilon < d/2$</span> where <span class=math >$d$</span> is the length of the interval. We have just shown that there are two points <span class=math >$\epsilon$</span> apart, so there is a point that is <span class=math >$x_{n+m}$</span> and <span class=math >$x_{n+k}$</span> which are <span class=math >$<\epsilon$</span> apart. Assuming WLOG <span class=math >$m>k$</span>, this means that <span class=math >$m-k$</span> rotations takes one from <span class=math >$x_{n+k}$</span> to <span class=math >$x_{n+m}$</span>, and so <span class=math >$m-k$</span> rotations is a rotation by <span class=math >$\epsilon$</span>. If we do <span class=math >$\frac{1}{\epsilon}$</span> rounded up rotations, we will then cover the space with intervals of length epsilon, each with one point of the orbit in it. Since <span class=math >$\epsilon < d/2$</span>, one of those intervals is completely encapsulated in <span class=math >$I$</span>, which means there is at least one point in our orbit that is in <span class=math >$I$</span>.</p> <p>Thus for every interval we have at least one point in our orbit that lies in it, proving that the rotation map with irrational <span class=math >$\alpha$</span> is dense. Note that during the proof we essentially showed as well that if <span class=math >$\alpha$</span> is rational, then the map is periodic based on the denominator of the map in its reduced form.</p> <h2>A Quick Note on Parallelism</h2> <p>Very quick note: all of these are hyper parallel since it does the same calculation per parameter or trajectory, and each calculation is long. For quasi-Monte Carlo, after generating &quot;good enough&quot; trajectories, one can evaluate the model at all points in parallel, and then simply do the GSA index measurement. For FAST, one can do each quadrature in parallel.</p> <div class=footer > <p> Published from <a href=global_sensitivity.jmd >global_sensitivity.jmd</a> using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.9 on 2024-04-09. </p> </div>